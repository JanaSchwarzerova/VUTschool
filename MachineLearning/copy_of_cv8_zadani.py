# -*- coding: utf-8 -*-
"""Copy of cv8_zadani.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sYVXTzKT6gNy1vpPeCVQvDDHzTBoirVd

## pridat feature importence u segmentace!
## Decision trees  - Classification And Regression Tree (CART)

Create predictor, which wil decide on all features simultaneusly is problematic. However, we can select one feature and decide based on it. Then inside already splited data we can select another feature and create next split, and continue until everything is predicted corectly. 

This make training much simpler, however, we do not know which feature to select to get optimal classifier. In case of decision tree, feature for every split is selected with greedy decision and simple treshhold is used as classifier.

### Pros
*   Simple
*   Interpretabe 
*   For any feature type - non-standardized, categorical....


### Cons
*   Small classification acuracy
*   Overfiting 


=> can be ovecam with baging/boosting => random forest

### Classification trees
"""

plt.plot??

import numpy as np
import matplotlib.pyplot as plt
import sklearn

from IPython.core.debugger import set_trace

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
from IPython.display import display

def plot_tree_iris(depth=1):


  # Parameters
  plot_colors = "rb"
  plot_step = 0.02

  # Load data
  iris = load_iris()

  # We only take the two corresponding features and two labels
  X = iris.data[:, [0,1]]
  y = iris.target

  X=X[y<2,:]
  y=y[y<2]

  
  clf = DecisionTreeClassifier(max_depth=depth).fit(X, y)


  plt.figure(figsize=(6, 6))

  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                       np.arange(y_min, y_max, plot_step))

  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

  Z = Z.reshape(xx.shape)
  cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)

  plt.xlabel(iris.feature_names[0])
  plt.ylabel(iris.feature_names[1])

  # Plot the training points
  for i, color in zip(range(2), plot_colors):
      idx = np.where(y == i)
      plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],
                  cmap=plt.cm.RdYlBu, edgecolor='black')

  plt.show()

  import graphviz 

  #     plt.figure(figsize=(12, 12))
  dot_data = sklearn.tree.export_graphviz(clf, out_file=None) 
  graph = graphviz.Source(dot_data) 
  graph.render("iris")   

  dot_data = sklearn.tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names[:2],  
                   class_names=iris.target_names[:2],  filled=True, rounded=True,special_characters=True)  
  
  graph=graphviz.Source(dot_data)
  graphviz.Source(dot_data).view()
  display(graph)


for k in range(1,5):
  plot_tree_iris(depth=k)

"""### Regression trees"""

import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

def plot_tree_iris(depth=1):

  # Create a random dataset
  rng = np.random.RandomState(1)
  X = np.sort(5 * rng.rand(80, 1), axis=0)
  y = np.sin(X).ravel()
  y[::5] += 3 * (0.5 - rng.rand(16))

  # Fit regression model
  regr_1 = DecisionTreeRegressor(max_depth=depth)
  regr_1.fit(X, y)

  # Predict
  X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
  y_1 = regr_1.predict(X_test)

  # Plot the results
  plt.figure()
  plt.scatter(X, y, s=20, edgecolor="black",
              c="darkorange", label="data")
  plt.plot(X_test, y_1, color="cornflowerblue",
           label="max_depth="+str(depth), linewidth=2)
  plt.xlabel("data")
  plt.ylabel("target")
  plt.title("Decision Tree Regression")
  plt.legend()
  plt.show
  
  
for k in range(1,10):
  plot_tree_iris(depth=k)

"""## Random forest

Prediction with many trees and averaging the results will prevent overfiting. 

But how to make every tree diferent? Bootstraping - of features and datapoints. Random subset of feature is selected at every tree node.

### Classification forest
"""

#### predict with 100 trees, every time with 2/3 of randomly selected datapoints
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
import numpy as np
import matplotlib.pyplot as plt

plot_colors = "rb"
plot_step = 0.02

# Load data
iris = load_iris()

# We only take the two corresponding features and two labels
X = iris.data[:, [0,1]]
y = iris.target

X=X[y<2,:]
y=y[y<2]


x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx1, xx2 = np.meshgrid(np.arange(x_min, x_max, plot_step),np.arange(y_min, y_max, plot_step))
xx1_vec=xx1.ravel()
xx2_vec=xx2.ravel()

yy_pred=np.zeros((xx1.shape[0],xx1.shape[1],100))


for k in range(100):###
  #vyberu náhodné vzorky x a y
   index = np.random.randint(0,len(X),int(len(X)*2/3))

   X_s = X[index,:]
   Y_s = y[index]

   clf=DecisionTreeClassifier()
   clf.fit(X_s,Y_s)

   XX = np.stack((xx1_vec,xx2_vec),axis=1)
   yy_pred[:,:,k]=clf.predict(XX).reshape(xx1.shape)

  #pass


vysl = np.mean(yy_pred, axis = 2)


plt.figure(figsize=(6, 6))
plt.contourf(xx1, xx2, vysl, cmap=plt.cm.RdYlBu)
# Plot the training points
for i, color in zip(range(2), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],
                cmap=plt.cm.RdYlBu, edgecolor='black')
plt.show()

"""- Create random forest classification and with 30 trees and aplly to previous examples.  Every tree with all features and 1/6 of samles with repetition. (Use sklearn classification tree sklearn.tree.DecisionTreeClassifier, but aregate them manually with for loop.)


- Aply it to segmentation of images (number of sample = number of pixels, R G B are features). Use one image for training and one for testing.
"""

from skimage.io import imread
import matplotlib.pyplot as plt
import numpy as np
from skimage.color import rgb2grey
from sklearn.ensemble import RandomForestClassifier


I_train=imread('2.png').astype(np.float64)/255
mask_train=np.round(rgb2grey(imread('2_GT.tif').astype(np.float64))).astype(np.int)
I_test=imread('4.png').astype(np.float64)/255
mask_test=np.round(rgb2grey(imread('4_GT.tif').astype(np.float64))).astype(np.int)

lbl_val=np.unique(mask_train)

X_train = I_train.reshape((-1,3))
y_train = mask_train.reshape((-1,1))

clf = RandomForestClassifier(20)
clf.fit(X_train,y_train)
res=clf.predict(I_test.reshape((-1,3))).reshape(mask_test.shape)

plt.imshow(I_train)
plt.show()
plt.imshow(mask_train)
plt.show()

plt.imshow(res)
plt.show()
plt.imshow(mask_test)
plt.show()

"""## Adaboost

There is exponetial number of weigths, how to combine set of week classifiars $h_t(x)$ into one strong classifier $H(x)$. Weak classifier can be anything - decision tree, SVM or human...  Adaboost is popular and succesful heuristical algorithm for selection of linear combination of week classifiers - for classes -1 and 1:

$H(x)=sign(\sum_{t=1}^{T}\alpha_t h_t(x))$


1.   Set uniform weights to all datapoints.

$D_i^{(1)}=\frac{1}{N}  ,\forall i$

2.   Add next classifier (best with consideration of weigths of datapoints)

$\epsilon_t =\sum_{n:h_t(x_n)\neq y_n} D_n^{(t)}$

$h_t =argmin_{h_j\in \mathcal{H}} \epsilon_t $

3.   Assign weigth to added classifier .

$\alpha_t=\frac{1}{2} ln(\frac{1-\epsilon_t }{\epsilon_t })$

4.   Change weights datapoint - inceras weights of incorectly classified and normalize.

$D_n^{(t+1)}=\frac{D_n^{(t+1)} exp(-\alpha_t y_n h_t(x_n))}{Z_t},$
$Z_t=\sum_{n=1}^N D_n^{(t+1)} exp(-\alpha_t y_n h_t(x_n))$


5.   Repet 2 - 4

Besides of combination of classifier, it can also be used as stand alone classifier with decision stumps (1 level trees) as weak classifier, which we can find online during trainig.

Create adaboost classifier with decision stumps for flowing dataset:
"""

import numpy as np
from numpy.random import rand
import matplotlib.pyplot as plt

n=200

angle=rand(n)*2*np.pi
l=rand(n)*40+30
blue=np.stack((np.sin(angle)*l,np.cos(angle)*l)).T

angle=rand(n)*2*np.pi
l=rand(n)*40
red=np.stack((np.sin(angle)*l,np.cos(angle)*l)).T

data=np.concatenate((red,blue),axis=0).astype(np.float64)
label=np.concatenate((-1*np.ones(n),np.ones(n)),axis=0).astype(np.float64)

ind=np.random.rand(len(label))<0.7

data_train=data[ind,:]
label_train=label[ind]
data_test=data[ind==0,:]
label_test=label[ind==0]


for k in np.unique(label):
  plt.plot(data_train[label_train==k,0],data_train[label_train==k,1],'*')

  

plt.show()



for k in np.unique(label):
  plt.plot(data_test[label_test==k,0],data_test[label_test==k,1],'*')

plt.show()

from sklearn.tree import DecisionTreeClassifier
import numpy as np
import matplotlib.pyplot as plt
from IPython.core.debugger import set_trace



# stump=DecisionTreeClassifier(max_depth=1)
# supmp.fit(X,y,sample_weight=D)

N,d=data_train.shape

D=np.ones(N)/N

it=500

X=data_train
y=label_train

stumps=[]
alphas=[]
for k in range(it):

  stump=DecisionTreeClassifier(max_depth=1)
  stump.fit(X,y,sample_weight=D)

  y_pred=stump.predict(data_train)
  bin = y_pred!=label_train
  e=np.sum(D[bin])  

  alfa = 1/2*np.log((1-e)/e)
  D= D*np.exp(-alfa * y * y_pred)/ np.sum(D)

  alphas.append(alfa)
  stumps.append(stump)


pred_final=np.zeros(label_test.shape)
for k in range(it):

  tmp = stumps[k].predict(data_test)
  pred_final=pred_final+alphas[k]*tmp

pred_final=np.sign(pred_final)

"""Create adaboost for mixing otput of unknown weak classifiers for flowing signal dataset:"""

from scipy.io import loadmat
import numpy as np
import matplotlib.pyplot as plt
from IPython.core.debugger import set_trace
np.random.seed(10)
    
a=np.load('ecg_class_adaboost.npz')    
a.files
data=a['arr_0'] ### ecg signal
label=a['arr_1']  #### gt classification
pred=a['arr_2']   ### prediction of 8 experts

plt.plot(data[:,label==-1])
plt.show()

plt.plot(data[:,label==1])
plt.show()


ind=np.random.randn(data.shape[1])<0.7
data_train=data[:,ind==1]
data_test=data[:,ind==0]  
label_train=label[ind==1]
label_test=label[ind==0] 
pred_train=pred[ind==1,:]
pred_test=pred[ind==0,:] 
  
  
for k in range(pred.shape[1]):
  print('expert'+ str(k) + ' accuracy:  '  +str(np.mean(label_test==pred_test[:,k])))

  
d,N=data_train.shape

D=np.ones(N)/N
  
it=500
X=data_train
y=label_train

clas_nums=[]
alphas=[]
for k in range(it):