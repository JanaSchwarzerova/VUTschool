# -*- coding: utf-8 -*-
"""Copy of cv4_zadani.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q5f4H6CsMIC8yWrCJtdPxJ88WN78nNMc

# Cvičení 4 - linear regression, polynomial regression, ridge/lasso regularization, one neuron, logistic regression

## Linear regression

Find a linear function that maps some features or variables to other variable $\mathbf{x} -> y$.

In 2D case with two datapoints, we need to find weigths whitch satisfy:


$w_1x_{11} + w_2x_{12}  = y_1$

$w_1x_{21} + w_2x_{22}  = y_2$

From $n$ $d$-dimensional datapoins, we can construc feature matrix $\mathbf{X}$ and response vector $\mathbf{y}$:

$\mathbf{X}=\left[
\begin{array}{ccc}
   x_{11} & \cdots & x_{1d} \\
   \vdots & \ddots & \vdots \\
   x_{n1} & \cdots & x_{nd}
\end{array}
\right]   
$
$
\mathbf{y}=\left[
\begin{array}{ccc}
   y_{1}\\
   \vdots \\
   y_{n} 
\end{array}
\right]
$

or we can generalize more to affine solution by introducing vector of ones, which will lead to bias term

$\mathbf{X}=\left[
\begin{array}{cccc}
  1& x_{11} & \cdots & x_{1d} \\
  \vdots & \vdots & \ddots & \vdots \\
   1&x_{n1} & \cdots & x_{nd}
\end{array}
\right]   
$

Optimal $\mathbf{w}$ than can be found as solution of $\mathbf{X} \mathbf{w} = \mathbf{y}$, however, this can be solved only if $\mathbf{X}$ is invertible ($d$ lineary independent rows).
"""

##### fit afine line with matrix inversion - plot points and final line
import numpy as np
import numpy.linalg as LA
import matplotlib.pyplot as plt
x11=1
x21=4
y1=2
y2=5



X=np.array([[1,x11],
            [1,x21]])
y=np.array([[y1],[y2]])
           
w = LA.inv(X)@y

plt.plot([x11,x21],[y1,y2],'.')


osa_x = np.linspace(0,5,100)
XX = np.stack((np.ones(100),osa_x),axis=1)

yy = XX@w

plt.plot(osa_x,yy)

"""But what we can do with system of equations without solution?

We can minimize error. This error can be defined for example as MSE:


$\underset{\mathbf{w}}{\text{mininize} } ||\mathbf{X}\mathbf{w}-\mathbf{y}||^2$


closed form solution:

$w^*= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$

this is also known as pseudo-inverse (avaliable in numpy).
"""

#### fit least squre afine line 
import numpy as np
import numpy.linalg as LA
import matplotlib.pyplot as plt

X=5*np.random.rand(100)
y=X*0.5+1+0.4*np.random.randn(100)

plt.plot(X,y,'.')
X=np.stack((np.ones(len(X)),X),axis=1)
w = (LA.inv(X.T@X))@(X.T)@y

osa_x = np.linspace(0,5,100)
XX = np.stack((np.ones(100),osa_x),axis=1)
yy = XX@w


plt.plot(osa_x,yy)

"""## Linear regression with feature trasformation (polynomial)

Similarly as we created afine fuction by addition ones to fo feature matrix, we can introduce more complex featrue transformations, most often to some spece of higher dimension $D$

$\Phi: R^d  \rightarrow  R^D$

and replace feature vectors in feature matrix by tranformed feature vectors 
$
\mathbf{X}=\left[
\begin{array}{ccc}
   \Phi(\mathbf{x_{1}})\\
   \vdots \\
   \Phi(\mathbf{x_{n}})\\
\end{array}
\right]
$

For example, second order polynomial feature transformation of 2D vector is
$\Phi([x_1,x_2])=[x_1^2,x_2^2,x_1 x_2,x_1,x_2,1]$


Polynomial feature matrix is also known as Vandermonde matrix.

$2^{nd}$ order Vandermonde for 1D case:




$\mathbf{X}=\left[
\begin{array}{ccc}
   x_{11}  \\
   \vdots  \\
   x_{n1} 
\end{array}
\right]   
->$
$\mathbf{V}=\left[
\begin{array}{ccc}
   x_{11}^2& x_{11}  & 1\\
   \vdots & \vdots & \vdots\\
   x_{n1}^2 & x_{n1} & 1
\end{array}
\right]   
$
"""

#### create function for 1D polynomial fitting  - use numpy.vander to generate Vandermode matrix
### explore overfiting with high order polynomial
import numpy as np
import numpy.linalg as LA
import matplotlib.pyplot as plt

X0=5*np.random.rand(40)
y=np.sin(X0)+0.3*np.random.randn(40)

plt.plot(X0,y,'.')
order=15
X = np.vander(X0,order) ##Toto je ještě training
w = (LA.inv(X.T@X))@(X.T)@y

X0 = np.linspace(0,5,100)
X = np.vander(X0,order) ##Toto je testing
yy = X@w

plt.plot(X0,yy)
plt.ylim([-2,2])

"""## Ridge regression

For higher dimenstios, it is hard to determine corect polynomial order.  Another way how to avoid overfiting is keep weigth values small with regularization. One posibility is again use L2 norm

$\underset{\mathbf{w}}{\text{mininize} } ||\mathbf{X}\mathbf{w}-\mathbf{y}||^2+\lambda||\mathbf{w}||_2^2$

which still have closed form solution

$w^*= (\mathbf{X}^T\mathbf{X}+\lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}$

$||\mathbf{x}||_p = (\sum_i|x_i|^p)^{1/p}$

$||\mathbf{x}||_2 = \sqrt{\sum_i(x_i)^2}$

$||\mathbf{x}||_2^2 = \sum_i(x_i)^2=\mathbf{x}^T \mathbf{x}$

$||\mathbf{x}||_1 = \sum_i|x_i|  $

Derivation of ridge regression.

$
L = (y-X\mathbf{w})^T(y-X\mathbf{w}) + \lambda\mathbf{w}^T\mathbf{w}=
$

$
= y^Ty - \mathbf{w}^TX^Ty - y^TX\mathbf{w}+ \mathbf{w}^Tx^TX\mathbf{w} + \lambda\mathbf{w}^T\mathbf{w}=
$

$
= y^Ty - \mathbf{w}^TX^Ty - \mathbf{w}^TX^Ty + \mathbf{w}^TX^TX\mathbf{w} + \mathbf{w}^T\lambda I\mathbf{w}=
$

$
= y^Ty - 2\mathbf{w}^TX^Ty + \mathbf{w}^T(X^TX + \lambda I)\mathbf{w}
$


$
\frac{\partial L }{\partial\mathbf{w}} =  -2X^Ty + 2(X^TX + \lambda I)\mathbf{w} =0
$

$
(X^TX + \lambda I)\mathbf{w} = X^Ty
$

$
 \mathbf{w}^* = (X^TX + \lambda I)^{-1} X^Ty
$
"""

##### modify previous code to ridge regression

import numpy as np
import numpy.linalg as LA
import matplotlib.pyplot as plt

X0=5*np.random.rand(40)
y=np.sin(X0)+0.3*np.random.randn(40)

plt.plot(X0,y,'.')
plt.ylim([-2,2])

order=10
lam=10
X = np.vander(X0,order)
w = (LA.inv(X.T@X+lam@np.eye(np.shape(X)[1])))@X.T@y
yy = X@w

X0 = np.linspace(0,5,100)
X = np.vander(X0,order)

plt.plot(X0,yy)
plt.ylim([-2,2])

"""## Lasso regression

L1 regularization will prefer sparse weigths. This can be view as feature selection, because some features will not be used.

$\underset{\mathbf{w}}{\text{mininize} } ||\mathbf{X}\mathbf{w}-\mathbf{y}||^2+\lambda||\mathbf{w}||_1$

Closed form solution does not exist, but it can be solved efectively with other methods like gradient descend. (It is convex problem, thus global minimum will be found.)
"""

#### use sklean lasso and ridge regression to diabetes dataset; compare sparisti of mdl.coef_
#### and find optimal lambda with crossvalidation - use np.logspace to search parametr with logaritmic  steps

import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge

diabetes = datasets.load_diabetes()
##Ten baseline variables, age, sex, body mass index, average blood pressure, 
##and six blood serum measurements were obtained for each of n = 442 diabetes patients,
##as well as the response of interest, a quantitative measure of disease progression one year after baseline.
####Note: Each of these 10 feature variables have been mean centered and 
####scaled by the standard deviation times n_samples (i.e. the sum of squares of each column totals 1).
X_train = diabetes.data[:150]
y_train = diabetes.target[:150]
X_test = diabetes.data[150:]
y_test = diabetes.target[150:]
print(diabetes.feature_names)
# print(diabetes.data[0,:])


alpha=0.1
# mdl=Lasso(alpha)
mdl=Ridge(alpha)
mdl.fit(X_train,y_train)

pred=mdl.predict(X_test)
w=mdl.coef_
rms=np.mean((y_test-pred)**2)

print(w)
print(rms)

"""## One neuron

###Linear neuron

---





One artificial neuron with linear activation finction $f(a)=a$ is equvalent of linear regression

$\hat{y}=f(\mathbf{x}^T\mathbf{w} + b)=f(\sum_{i=1}^p x_i w_i + b) = wx+b$

Optimal weigths can be found with gradient descend. In 1D case, update of weigths is:

$w = w - \mu \frac{\partial L}{dw}$

$b = b - \mu \frac{\partial L}{db}$

<br>

WIth MSE loss 

$L(\theta) = \frac{1}{2} (\hat{y} -y)^2 = \frac{1}{2} (w x +b -y)^2$

$\frac{\partial L}{dw}= (wx+b-y) x$

$\frac{\partial L}{db}= (wx+b-y)$
"""

## create function for stochastic gradient desced optimization of one neuron
import numpy as np
import numpy.linalg as LA
import matplotlib.pyplot as plt
from IPython.core.debugger import set_trace

X=5*np.random.rand(100)
y=X*0.5+2+0.4*np.random.randn(100)
##X=X[r]
##y=y[r]
plt.plot(X,y,'.')
plt.show()
mu=0.0005
w=0.1
b=-0.1


for i in range(300):
  for ii in range(lam(X)):
    y = w@X[ii]+b
    der_Lw = (w@x[ii]+b-y)@x
    der_Lb = (w@x+b-y)
    w = w - mu@der_Lw
    b = b - mu@der_Lb

"""###Sigmoidal neuron

$a=xw+b$

$\hat{y}=f(a)= \frac{1}{1+e^{-a}} $


$\frac{\partial f}{\partial a}=f(a)(1- f(a))$


$\frac{\partial L}{dw}= (f(a)-y)\frac{\partial f}{\partial a}  x$

$\frac{\partial L}{db}= (f(a)-y)\frac{\partial f}{\partial a}$
"""

import numpy as np
import numpy.linalg as LA
import matplotlib.pyplot as plt

X=np.concatenate((np.random.rand(50)+1.1,np.random.rand(50)+2),axis=0)
y=np.concatenate((np.zeros(50),np.ones(50)),axis=0)
r=np.random.permutation(100)
X=X[r]
y=y[r]



plt.plot(X,y,'.')
plt.show()

mu=0.1


w=0.1
b=-0.1

"""## Logistic regression"""

### apply sklearn logistic regression to same dataset
from sklearn.linear_model import LogisticRegression
import numpy as np

X=np.concatenate((np.random.rand(50)+1.1,np.random.rand(50)+2),axis=0)
y=np.concatenate((np.zeros(50),np.ones(50)),axis=0)

X=X.reshape(-1,1)


plt.plot(X,y,'.')
plt.show()

### apply sklearn logistic regression to iris dataset

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression

iris = datasets.load_iris()

y= iris.target
X = iris.data[y<2,0:2]#### select just 2 features and 2 classes
y =y[y<2]
ind=np.random.rand(len(y))>0.2

X_train=X[ind,:]
y_train=y[ind]
X_test=X[ind==0,:]
y_test=y[ind==0]

clf = LogisticRegression()

 




 
xx1,xx2=np.meshgrid(np.linspace(4,7.5,100),np.linspace(1.5,4.5,100))
xx=np.stack((xx1.ravel(),xx2.ravel()),axis=1)