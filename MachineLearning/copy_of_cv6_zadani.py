# -*- coding: utf-8 -*-
"""Copy of cv6_zadani.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19cvAfdnS_MGfQH4xR0A65tuqOybCbfOo

#Cvičení 6 - Bayes decision theory

# Maximum a posteriory estimation - MAP

$\theta^* = argmax_{\theta} p(\theta\mid x)$

Find the most probable model parameters for given data.



# Maximum likelihood estimation - MLE

$\theta^* = argmax_{\theta} p( x\mid\theta)$

Under the assumed statistical model the observed data is most probable.



# Bayes rule

$ p(\theta \mid x) =\frac{p(\theta ) p( x \mid \theta)}{p( x)}\propto p(\theta ) p( x \mid \theta) $

### Gaussian MLE - analytical solution

$p(x \mid \mu,\sigma^2 )=  \frac{1}{\sigma \sqrt{2 \pi}} exp(\frac{-(x-\mu)^2}{2\sigma^2})$

MLE solution:

$\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}$

$\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n( x_i - \bar{x})^2 = std(x)^2$
"""

import numpy as np
import matplotlib.pyplot as plt

np.random.seed(24)
x=5*np.random.randn(80)+5

plt.plot(x,np.ones(80)*0.01,'*')
plt.hist(x,10,density=1)
plt.show()

xx=np.linspace(-5,15,500)


mu = np.mean(x)
sigma = np.std(x)

G = (1/(sigma*np.sqrt(2*np.pi)))*np.exp(-(xx-mu)**2/(2*sigma**2))

plt.plot(xx,G,'*')
plt.hist(x,10,density=1)
plt.show()

xx=np.linspace(-5,15,500)

"""### Bernouli MLE - analytical solution

Binary random variable.

$p(x \mid \theta )=\theta^{x} (1-\theta)^{(1-x)} $

$0=\frac{dlogP(\mathbf{x}=x_1,x_2,...,x_n \mid \theta )}{d\theta}= ? $

$\hat{\theta}=?$

$\hat{\theta} = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}$

Find probabilty model for this vector of i.i.d. Bernouli samples and generate 20 more samples:
"""

import numpy as np
x=np.array([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 1])

theta = np.mean(x)

xx = np.random.randn(20)>theta 

plt.hist(1-theta,2,density=1)
plt.hist(theta,2,density=1)
plt.show()


print(xx)

"""### Gaussian MLE - numerical solution - brute-force

Grid search over parameter space.

$log ( p(x_1,x_2,..,x_n|\mu,\sigma) )=\sum_{i=1}^n -log(\sqrt{2 \pi \sigma^2})-\frac{1}{2 \sigma^2} (x_i-\mu)^2$
"""

import numpy as np
import matplotlib.pyplot as plt

np.random.seed(24)
x=5*np.random.randn(80)+5

steps=100
loglikelihood=np.zeros((steps,steps))
mus=np.linspace(1,10,steps)
sigs=np.linspace(3,10,steps)

posx,posy= np.where(loglikelihood==np.max(loglikelihood))

for i in range(steps): 
  for ii in range(steps):
    G = (1/(sigs[i]*np.sqrt(2*np.pi)))*np.exp(-(x-mus[ii])**2/(2*sigs[i]**2))
    loglikelihood[i,ii] = np.sum(np.log(G))

posx,posy= np.where(loglikelihood==np.max(loglikelihood))

plt.imshow(loglikelihood)
plt.show(posy, posx)
plt.show()

"""### Bernouli MAP - analytical solution

$\theta^*_{MAP} = argmax_{\theta} p( \theta\mid x_1,x_2,..x_n)=  argmax_θ p(  x_1,x_2,..x_n \mid \theta) p(\theta )$

$x_i \sim Bernoulli(\theta)$

$\theta \sim Beta(\alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{1-\alpha}(1-\theta)^{1-\beta}$


$ \Gamma(n)=(n-1)!$

$=>\theta_{MAP}= \frac{\sum_{n=1}^n x_i +\alpha -1}{n + \beta + \alpha -2}$

Calculate MLE and MAP estimate pro počet slovenských studentů ve třídě:
"""

cesi=35
slovaci=15

alpha=2
beta=9.74

theta_MLE = slovaci/(cesi+slovaci)
theta_MAP = (slovaci+alpha-1)/(cesi+slovaci+beta+alpha-2)

print('MLE = ' + str(theta_MLE))
print('MAP = ' + str(theta_MAP))

"""Generate Beta prior pro výskyt slovenských studentů na českých vysokých školách  $Beta(2,9.74)$:"""

import numpy as np
import matplotlib.pyplot as plt

def beta_dist(a,b,x): 
  from scipy.special import gamma
  p=gamma(a+b)/(gamma(a)*gamma(b))*x**(a-1)*(1-x)**(b-1)
  return p

new_beta = beta_dist(alpha,beta,slovaci)

plt.plot(new_beta)
plt.show()

x=np.linspace(0,1,100)

"""## Bernouli posterior

For conjugate priors, we can derive analytical solution for whole posterior distribution.

https://en.wikipedia.org/wiki/Conjugate_prior

$Beta(\alpha_2,\beta_2)=Bernoulli(\theta) Beta(\alpha,\beta)$

$ \alpha_2= \alpha +\sum _{i=1}^{n}x_{i}$

$\beta_2=\beta +n-\sum _{i=1}^{n}x_{i}$
"""

### calculate and plot prior distribution and posterior distribution


from scipy.special import gamma
import numpy as np
import matplotlib.pyplot as plt

def beta_dist(a,b,x): 
  p=gamma(a+b)/(gamma(a)*gamma(b))*x**(a-1)*(1-x)**(b-1)
  return p
  

x=np.linspace(0,1,100)

"""# Bayes Classifier
We want to find most probable class given data we obsere:
$y_k^* = argmax_{y_k} P(Y=y_k \mid X1=x_i,X2=x_j) = argmax_{y_k} \frac{P(Y=y_k ) P( X1=x_i,X2=x_j\mid Y=y_k)}{P( X1=x_i,X2=x_j)}=$

$= argmax_{y_k} P(Y=y_k ) P( X1=x_i,X2=x_j \mid Y=y_k )     $

For two features (or few features) and discrete distribution, we can find this distribution by simple counting in dataset $D$:
"""

import pandas as pd
import numpy as np


X1=np.array([0,1,0,1,1,0,1])
X2=np.array([0,1,1,1,0,0,1])
Y=np.array([0,1,1,1,1,1,0])

"""then  probability of $P(Y= y_k)$ can be find simply as frequency of this class - e.g. $P(Y= 1) = \frac{\#D\{Y=1\}}{|D|}$ :"""

p1=np.sum(Y==1)/np.size(Y)
print(p1)

"""and $ P(X1=x_i,X2=x_j |Y=y_k)$ is frequency of $x_j$ feature in samples with  $y_k$ - e.g. $ P(X1=1,X2=1|Y=1)=\frac{\#D\{X1=0 \wedge X2=1 \wedge Y=1\}}{\#D\{Y=1\}}$"""

p2=np.sum((X1==1)&(X2==1)&(Y==1))/np.sum(Y==1)
print(p2)

"""Calculate all probabilites  $p(k,j,i)$ (build a classifier) and predict new sample $x_{new}=[0,0]$:"""

p_x1_x2_y=np.zeros((2,2,2))
p_y=np.zeros(2)

for i in range(steps): 
  for ii in range(steps):
    p_y=np.zeros(2)