# -*- coding: utf-8 -*-
"""Copy of cv5_zadani.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1grxxbAVz6SFFKkNrwoX5eox0k-7HXMkw

#Cvičení 5 - kernel methods - kernel regression, SVM, kernel SVM, kernel PCA

## Kernel ridge regression (kernel least-squares)
"""

## numerical "proof" 
import numpy as np
from numpy.linalg import inv

X=np.random.rand(3,2)
y=np.random.rand(3,1)

print(inv(X.T@X+np.eye(2))@X.T@y)

print(X.T@inv(X@X.T+np.eye(3))@y)

"""### Primal form

$\underset{\mathbf{w}}{\text{mininize} } ||\mathbf{X}\mathbf{w}-\mathbf{y}||^2+\lambda||\mathbf{w}||^2$

closed form solution:

$w^*= (\mathbf{X}^T\mathbf{X}+\lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}$


$\hat{y} = \mathbf{X} \mathbf{w}$

### Dual form

It can be rewrite as (if $\lambda >0$)

$w^*=\mathbf{X}^T (\mathbf{X}\mathbf{X}^T+\lambda \mathbf{I})^{-1}\mathbf{y}$

$w^*$ is clearly a linear combination of training examples 

$\mathbf{\alpha} = (\mathbf{X}\mathbf{X}^T+\lambda \mathbf{I})^{-1}\mathbf{y}$

$\mathbf{w}=\sum_{i=1}^N \alpha_i \mathbf{x_i}$


then prediction of new example $\mathbf{x}$ can be done by


$\hat{y} = \sum_{i=1}^N \alpha_i \mathbf{x_{new}} \mathbf{x_i}^T=\mathbf{\alpha^TX\mathbf{x_{new}^T}} $

Why is dual form better? It works id $n$ dimensional space instead of $d$-dimensinal space (we can choose smaler dimension). And we can aply kernel trick.

$\underset{\mathbf{\alpha}}{\text{mininize} } ||\mathbf{X}\mathbf{w}-\mathbf{y}||^2+\lambda||\mathbf{w}||^2$
"""

#### first implement ridge regression in dual form
#### modify dual ridge regression to kernel regression

import numpy as np
import numpy.linalg as LA
import matplotlib.pyplot as plt
from IPython.core.debugger import set_trace
from sklearn.metrics.pairwise import euclidean_distances


X0=5*np.random.rand(40)
y=np.sin(X0)+0.3*np.random.randn(40)
X=X0.reshape(-1,1)
y=y.reshape(-1,1)
plt.plot(X0,y,'.')
lam = 0.1

## w = (LA.inv(X@X.T)+lam@np.eye(np.shape(X)[2]))@(X.T)@y
alfa = inv(X@X.T+lam*np.eye(np.shape(X)[1]))@y
alfa = alfa.reshape(-1,1)

X_line = np.linspace(0,5,100)
X_new = X_line.reshape(-1,1)

y = []
for i in range(np.shape(X_new)[0]): 
  y.append(alfa.T@X@X_new[[i],:])

plt.plot(X_line,np.array(y).ravel())

"""### Kernel trick

Standarad feature tranfromation is

$\Phi: R^d  \rightarrow  R^D$

which can increase computatinel time, espetialy when $d<<D$.

<br>

Kernels are special funtions, which produces dot-products of transformed features:

$k_{ij}=k(\mathbf{x_i},\mathbf{x_j})=\Phi(\mathbf{x_i})^T\Phi(\mathbf{x_j})$.

without need to calculate this dot-product, but it produce $k_{i,j}$ directly - without need to create D-dimensional space and perform dot-product there.

There is many standard kernel functions - mostu useful is polynomial kernel (equivalent to polynomial feature transformation) and Gaussian kernel.

If we want to apply kernel trick, we need to transform are problem to the form, without original features $\mathbf{x}$ (row vector), but only its dot-products $\mathbf{x}^T\mathbf{x}$. Then we can simply replace $\mathbf{x}^T\mathbf{x} \rightarrow k(\mathbf{x},\mathbf{x})$.

Kernel matrix - is equivalent of $\Phi(\mathbf{X})\Phi(\mathbf{X})^T$:

$
K = \left[
\begin{array}{ccc}
   k(\mathbf{x_1},\mathbf{x_1}) & \cdots & k(\mathbf{x_1},\mathbf{x_n}) \\
   \vdots & \ddots & \vdots \\
   k(\mathbf{x_n},\mathbf{x_1})  & \cdots & k(\mathbf{x_n},\mathbf{x_n}) 
\end{array}
\right]   
$

Dual from  of linear regression requre only dot-products for both training and predition => we can aply kernel trick.

### Polynomial kernel

$k(\mathbf{x},\mathbf{y}) = (\mathbf{x}^\mathsf{T} \mathbf{y}+ 1)^{p}$

$k(\mathbf{x},\mathbf{y}) =  \Phi(\mathbf{x})^T \Phi(\mathbf{y}) $

For $p=2$:

$ k(\mathbf{x},\mathbf{y}) =\left(\sum _{i=1}^{n}x_{i}y_{i}+1\right)^{2}=$

$=\sum _{i=1}^{n}\left(x_{i}^{2}\right)\left(y_{i}^{2}\right)+\sum _{i=2}^{n}\sum _{j=1}^{i-1}\left({\sqrt {2}}x_{i}x_{j}\right)\left({\sqrt {2}}y_{i}y_{j}\right)+\sum _{i=1}^{n}\left({\sqrt {2}}x_{i}\right)\left({\sqrt {2}}y_{i}\right)+1$


$
\Phi(\mathbf{x}) = [ x_n^2, \ldots, x_1^2, \sqrt{2} x_n x_{n-1}, \ldots, \sqrt{2} x_n x_1, \sqrt{2} x_{n-1} x_{n-2}, \ldots, \sqrt{2} x_{n-1} x_{1}, \ldots, \sqrt{2} x_{2} x_{1}, \sqrt{2c} x_n, \ldots, \sqrt{2} x_1, 1]
$

which is polynomial feature trasform as we have used before.

Matrix form:



$K = (\mathbf{X}\mathbf{Y}^\mathsf{T} + 1)^{p}$

### Kernel regression

$\mathbf{\alpha} = (\mathbf{K}+\lambda \mathbf{I})^{-1}\mathbf{y}$


then prediction of new example $\mathbf{x}$ can be done by


$\hat{y} = \sum_{i=1}^N \alpha_i k(\mathbf{x_{new}}, \mathbf{x_i})=\mathbf{\alpha^T}\mathbf{K_{new}} $
"""

###### copy dual regresion
###### and modify it to polynomial regression using kernel trick
### first define polynomial kernel function K_poly, then replace all 'dot-products' X@X.T with K_poly(X,X)

import numpy as np
import numpy.linalg as LA
import matplotlib.pyplot as plt
from IPython.core.debugger import set_trace
from sklearn.metrics.pairwise import euclidean_distances


X0=5*np.random.rand(40)
y=np.sin(X0)+0.3*np.random.randn(40)
X=X0.reshape(-1,1)
y=y.reshape(-1,1)

plt.plot(X0,y,'.')
p = 6  #stupeň
lam = 0.01

def K_polynom(x1,x2,x3):
  k = (x1@x2.T+1)**x3
  return k

def K_reg(x1,x2,gama):
  eukl_dist = sqrt(x1*x1 + x2*x2)
  k = exp(-gama*eukl_dist)
  return k

K = K_polynom(X,X,p)
alfa = inv(K+lam*np.eye(np.shape(X)[1]))@y
alfa = alfa.reshape(-1,1)

X_line = np.linspace(0,5,100)
X_new = X_line.reshape(-1,1)

y = []
for i in range(np.shape(X_new)[0]): 
  K = K_polynom(X,X_new[[i],:],p)
  y.append(alfa.T@K)

plt.plot(X_line, np.array(y).ravel())

"""### RBF kernel regression

$ k(\mathbf {x} ,\mathbf {y} )=\exp(-\gamma \|\mathbf {x} -\mathbf {y} \|^{2})$


$K=exp(-\gamma \ euclidean\_distances(\mathbf {X},\mathbf {Y})^{2})$

Add RBF kernel to previous code.

## SVM

### Primal form

training:

$\underset{\mathbf{w}}{\text{mininize} } \:  \frac{1}{2}\mathbf{w}^T\mathbf{w} + c \sum_{j} \xi_j$

$\text{subject to} \: (\mathbf{w}^T\mathbf{x_j}+b)y_j \geq 1- \xi_j$ 

prediction:

 $\hat{y}=\text{sign} (\mathbf{w}^T\mathbf{x_{new}}+b )$

### Dual form

training:

$\underset{\mathbf{\alpha}}{\text{maximize} } \:\sum_{i=1}^N \alpha_i  \sum_{i=1}^N\sum_{j=1}^N  \alpha_i -\frac{1}{2}  \alpha_j  y_i   y_i  \mathbf{x_i}^T\mathbf{x_j}$

$\text{subject to} \: 0 \leq \alpha_i \leq c$

$\: \: \:\: \: \: \: \: \: \: \: \: \: \: \: \: \: \: \:  \sum_{i=1}^N \alpha_i y_i =0$

prediction:

 $\hat{y}=\text{sign} ( \sum_{k \in  SV} \alpha_k y_k \mathbf{x}_{new}^T\mathbf{x}_k+b )$
 
 
 ($\alpha$ is 0 for non-suport vectors)

### Kernelized SVM

training:

$\underset{\mathbf{\alpha}}{\text{maximize} } \:\sum_{i=1}^N \alpha_i  \sum_{i=1}^N\sum_{j=1}^N  \alpha_i -\frac{1}{2}  \alpha_j  y_i   y_i  k(\mathbf{x_i},\mathbf{x_j})$

$\text{subject to} \: 0 \leq \alpha_i \leq c$

$\: \: \:\: \: \: \: \: \: \: \: \: \: \: \: \: \: \: \:  \sum_{i=1}^N \alpha_i y_i =0$

prediction:

 $\hat{y}=\text{sign} ( \sum_{k \in  SV} \alpha_k y_k k(\mathbf{x}_{new},\mathbf{x}_k)+b )$
 
 
 ($\alpha$ is 0 for non-suport vectors)
"""

##### use sklearn for classification of iris dataset - create a grid - predict and plot, visualize suport vectors ,   explotre w,alpha,  
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC as SVM


iris = datasets.load_iris()

y= iris.target
X = iris.data[y<2,0:2]#### select just 2 features and 2 classes
y =y[y<2]
y[y==0]=-1
ind=np.random.rand(len(y))>0.2

X_train=X[ind,:]
y_train=y[ind]
X_test=X[ind==0,:]
y_test=y[ind==0]

svm = SVM(kernel= 'poly',C=10,degree=3)
svm.fit(X_train,y_train)
yy=svm.predict(X_test)

xx1,xx2 = np.meshgrid(np.linspace(0,8,100),np.linspace(1,5,100))
xx = np.stack((xx1.ravel(),xx2.ravel()),axis=1)

plt.contourf(xx1,xx2,yy,alpha=0.6)
plt.scatter(X_test[:,0],k_test[:,1],c=y_test)
plt.show()

"""### RBF SVM"""

from numpy.random import rand
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC as SVM

np.random.seed(5)
n=200
angle=rand(n)*2*np.pi
l=rand(n)*40+30
blue=np.stack((np.sin(angle)*l,np.cos(angle)*l)).T

angle=rand(n)*2*np.pi
l=rand(n)*40
red=np.stack((np.sin(angle)*l,np.cos(angle)*l)).T

data=np.concatenate((red,blue),axis=0).astype(np.float64)
label=np.concatenate((-1*np.ones(n),np.ones(n)),axis=0).astype(np.float64)*-1

ind=np.random.rand(len(label))<0.7

X_train=data[ind,:]
y_train=label[ind]
X_test=data[ind==0,:]
y_test=label[ind==0]

"""## Kernel PCA

### Primal form

$\mathbf{C}=\frac{1}{N}\mathbf{X}^{\top }\mathbf{X}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{x_i} \mathbf{x_i}^T$
 
Eigenvalue decomposition:

 $\lambda {\mathbf  {w}}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{x_i} \mathbf{x_i}^T{\mathbf  {w}}$
 
Projection of new datapoint $\mathbf{x}$ it $k$-th PC:
 
$\mathbf{t_k} = \mathbf{x}^T\mathbf{w_k}$

$\mathbf{T}= \mathbf{X}\mathbf{W}$

### Dual form

$\mathbf{C}'=\frac{1}{N}\mathbf{X}\mathbf{X}^{\top }=\frac{1}{N}\sum_{i=1}^{N}\mathbf{x_i}^T \mathbf{x_i}$

Eigenvalue decomposition:

 $N\lambda {\mathbf  {a}} =\sum_{i=1}^{N}\mathbf{x_i}^T \mathbf{x_i} \mathbf {a}$

Projection of new datapoint $\mathbf{x}$ to $k$-th PC:

$\mathbf{t_k} = \sum_{i=1}^{N} {a_{ik}}{\mathbf  {x}}^{T}{\mathbf  {x_i}} $

$\mathbf{T}= \mathbf{C}'\mathbf{a}$


### Kernel PCA
 $N\lambda {\mathbf  {a}} =K' \mathbf {a}$


Data need to be zero-centered in the kernel space. This can be achived by simple modification of kernel:


$K'=K-{\mathbf  {1_{N}}}K-K{\mathbf  {1_{N}}}+{\mathbf  {1_{N}}}K{\mathbf  {1_{N}}}$

Where ${\mathbf  {1_{N}}}$ is NxN matrix with $1/N$ entries.

Projection of a new datapoint $\mathbf{x}$ to $k$-th principal component is:

$\mathbf{t_k} =\Phi(\mathbf{x})^T\mathbf{w_k}= \sum_{i=1}^{N} {a_{ik}}\Phi ({\mathbf  {x}})^{T}\Phi ({\mathbf  {x_i}}) =\sum_{i=1}^{N} {a_{ik}} K'_i$

where $\mathbf{x_i}$ are original datapoints.


$\mathbf{T}= K'\mathbf{a}$
"""

from sklearn.decomposition import KernelPCA
from numpy.random import rand

np.random.seed(5)
n=200
angle=rand(n)*2*np.pi
l=rand(n)*40+30
blue=np.stack((np.sin(angle)*l,np.cos(angle)*l)).T

angle=rand(n)*2*np.pi
l=rand(n)*40
red=np.stack((np.sin(angle)*l,np.cos(angle)*l)).T

rot_mat=np.array([[2*np.cos(np.pi/4),-np.sin(np.pi/4)],[np.sin(np.pi/4),np.cos(np.pi/4)]])
X=np.concatenate((red,blue),axis=0).astype(np.float64)@rot_mat.T
y=np.concatenate((-1*np.ones(n),np.ones(n)),axis=0).astype(np.float64)*-1



plt.scatter(X[:,0],X[:,1],c=y)
plt.xlim([-120,120])
plt.ylim([-120,120])
plt.show()

##### calculate KPCA with eigenvalu decompositon
##### use kernel modification to zero-center
import numpy as np
import numpy.linalg as LA
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import euclidean_distances
from numpy.random import rand




np.random.seed(5)
n=200
angle=rand(n)*2*np.pi
l=rand(n)*40+30
blue=np.stack((np.sin(angle)*l,np.cos(angle)*l)).T

angle=rand(n)*2*np.pi
l=rand(n)*40
red=np.stack((np.sin(angle)*l,np.cos(angle)*l)).T

rot_mat=np.array([[2*np.cos(np.pi/4),-np.sin(np.pi/4)],[np.sin(np.pi/4),np.cos(np.pi/4)]])
X=np.concatenate((red,blue),axis=0).astype(np.float64)@rot_mat.T
y=np.concatenate((-1*np.ones(n),np.ones(n)),axis=0).astype(np.float64)*-1


plt.scatter(X[:,0],X[:,1],c=y)
plt.xlim([-120,120])
plt.ylim([-120,120])
plt.show()