# -*- coding: utf-8 -*-
"""Copy of cv9_zadani.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1irauwBTlBNzaLE4k3vZU8s4_nHum7mqI

# Cvičení 9 - jeden neuron, jednoducháneuronavá síť, úvod do pytorch

## Regression with simple neural network using numpy
"""

import numpy as np
import matplotlib.pyplot as plt
from IPython.core.debugger import set_trace


rng = np.random.RandomState(1)
X = 5 * rng.rand(150)
Y = np.sin(X).ravel()
Y[::10] += 3 * (0.5 - rng.rand(15))
Y=Y+0.1*np.random.randn(150)

plt.plot(X,Y,'*')
plt.show()

n=1000           
mu = 0.01  
std=0.1
W1=std*np.random.randn(n,1) #udělá nám to celou vrstvu neuronu
#inicializovat váhy v první vrstvě 1000, v druhé vrstvě 1000 a ve třetí vrstvě 1
b1=std*np.random.randn(n,1)                       

W2=std*np.random.randn(n,n)
b2=std*np.random.randn(n,1) 

W3=std*np.random.randn(1,n)
b3=std*np.random.randn(1,1) 

for k in range(10000):
  ind=np.random.randint(0,150)

  x=X[ind].reshape((1,1))
  y=Y[ind].reshape((1,1))

  h0=x

  a1=W1@h0+b1
  h1=a1*(a1>0)
  a2=W2@h1+b2
  h2=a2*(a2>0)
  a3=W3@h2+b3
  h3=a3

  y_hat = h3

  L=0.5*(y_hat-y)**2
  g_h3=y_hat-y       #note viz slaid
  g_a3=g_h3       # *1 ... derivace aktivační fce je 1 (aktivační fce je lineární fce)
  g_h2=W3.T@g_a3
  g_W3=g_a3@h2.T
  g_b3=g_a3

  g_a2=g_h2*(a2>0)      
  g_h1=W2.T@g_a2
  g_W2=g_a2@h1.T
  g_b2=g_a2

  g_a1=g_h1*(a1>0)      
  g_h0=W1.T@g_a1
  g_W1=g_a1@h0.T
  g_b1=g_a1

  W1=W1-mu*g_W1
  b1=b1-mu*g_b1
  W2=W2-mu*g_W2
  b2=b2-mu*g_b2
  W3=W3-mu*g_W3
  b3=b3-mu*g_b3

  if k%50==0: #jednou za padesát iterací se udělá další for cyklus
      xx=np.linspace(0.5,4.5,200)
      yy=[]
      for kk in range(len(xx)):
        x=xx[kk].reshape((1,1))
        h0=x
        a1=W1@h0+b1
        h1=a1*(a1>0)
        a2=W2@h1+b2
        h2=a2*(a2>0)
        a3=W3@h2+b3
        h3=a3
        
        y_hat = h3
        yy.append(y_hat)
      
      plt.plot(X,Y,'*')
      plt.plot(xx,np.array(yy).ravel(),'*')
      plt.show()

"""## Basic operations with pytorch"""

import numpy as np
import torch

#### torch tensor can be created similarly to numpy array - with randn, ones, zeros....
a = torch.randn(5, 7, dtype=torch.double)
print(a)
print(a.size())

a=np.random.randn(5,7).astype(np.float32) ## float32/single - is most popular format for pytorch tensors

a=torch.from_numpy(a)  ### tensor can be created by conversion from numpy arrays

b = a.numpy() ## and back
print(b)

##### example of automatic diferentiation

x = torch.ones(2, 2, requires_grad=True) ###set requires_grat to true - pytorch will automaticaly track gradients for this tensor

y=x*5

loss=torch.sum(y)


loss.backward() ###calculate gradients of loss with respect to all tensors which requires gradient
## loss need to be 1x1 tensor!!!!!


print(x.grad) ### use .grad to acces gradients of tensors 


x.grad.zero_() #### clear gradients  - otherwise gradients are acumulated in next calculations


print(x.grad)

#### do something without tracking of gradients
x = torch.ones(2, 2, requires_grad=True) 
print((x * 5).requires_grad)

with torch.no_grad():
  print((x * 5).requires_grad)
  
  
  
x=x.detach()   # disconect tensor from previous computations - it stops gradient propagation
print((x * 5).requires_grad)

### check cuda availability
print('available cuda? ' + str(torch.cuda.is_available()))   ##### setup cuda in notebook setting - edit/nootebook setting and select GPU

#### tranfer your operation to GPU to perform calculations faster

device = torch.device("cpu")
# device = torch.device("cuda:0")
x = torch.ones(2, 2, requires_grad=True,device=device) ###set device to cuda and create cuda tenosr
                 
# x = torch.ones(2,2)  ### or trasfer tensor to cuda and set requires_gradient afterwards
# x=x.to(device)  
# x.requires_grad=True


# x = torch.ones(2,2)  ### or trasfer tensor to cuda and set requires_gradient afterwards
# x=x.cuda()  
# x.requires_grad=True


y=x*5

loss=torch.sum(y)


loss.backward() 


print(x.grad) 


print(loss.cpu()) ###back to cpu

print(loss.cpu().detach().numpy())  ### back to cpu and numpy

"""## Regression with simple neural network using pytorch

Transform previous code to pytorch:

*   change numpy arrays to tensors
*   get gradients with automatic diferentiation
"""

import numpy as np
import torch
import matplotlib.pyplot as plt
from IPython.core.debugger import set_trace


rng = torch.random.get_rng_state(1)
X = 5 * rng.rand(150)
Y = np.sin(X).ravel()
Y[::10] += 3 * (0.5 - rng.rand(15))
Y=Y+0.1*torch.from_np.random.randn(150)



n=1000           
          
mu = 0.01  
std=0.1
W1=std*torch.random.randn(n,1) #udělá nám to celou vrstvu neuronu
#inicializovat váhy v první vrstvě 1000, v druhé vrstvě 1000 a ve třetí vrstvě 1
b1=std*torch.random.randn(n,1)                       

W2=std*torch.random.randn(n,n)
b2=std*torch.random.randn(n,1) 

W3=std*torch.random.randn(1,n)
b3=std*torch.random.randn(1,1) 

for k in range(10000):
  ind=torch.random.randint(0,150)

  x=X[ind].reshape((1,1))
  y=Y[ind].reshape((1,1))

  h0=x

  a1=W1@h0+b1
  h1=a1*(a1>0)
  a2=W2@h1+b2
  h2=a2*(a2>0)
  a3=W3@h2+b3
  h3=a3

  y_hat = h3

  L=0.5*(y_hat-y)**2
  L.backward()

  with torch.no_grad():
      W1=mu*W1.grad
      b1=mu*b1.grad
      W2=mu*W2.grad
      b2=mu*b2.grad
      W3=mu*W3.grad
      b3=mu*b3.grad

  W1.grad.zero_()
  b1.grad.zero_()
  W2.grad.zero_()
  b2.grad.zero_()
  W3.grad.zero_()
  b3.grad.zero_()


  if k%50==0: #jednou za padesát iterací se udělá další for cyklus
      xx=np.linspace(0.5,4.5,200)
      yy=[]
      for kk in range(len(xx)):
        x=xx[kk].reshape((1,1))
        h0=x
        a1=W1@h0+b1
        h1=a1*(a1>0)
        a2=W2@h1+b2
        h2=a2*(a2>0)
        a3=W3@h2+b3
        h3=a3
        
        y_hat = h3
        yy.append(y_hat)
      
      plt.plot(X,Y,'*')
      plt.plot(xx,torch.array(yy).ravel(),'*')
      plt.show()



"""## Inheritance

Inheritance allows us to define a class that inherits all the methods and properties from another class.
"""

class ParentClass():
  def __init__(self,q):
    self.parentAtribute = q

  def parentMethod(self):

    print('parentMethod')
    
    
class ChildClass(ParentClass):
  def __init__(self,q):
#     ParentClass.__init__(self, q)
    super().__init__(q)
    
    self.childAtribute=5

  def childMethod(self):
    print('childMethod')    
    
    
child=ChildClass(5)

child.parentMethod()

child.childMethod()

print(child.parentAtribute)

"""## nn.Module

Class `torch.nn.Module` contains basic operations of torch neural networks. 


Base class for all neural network modules.

Your models should also subclass this class.

Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes. 

For construction of a network you need to inherite module class and overwrite `__init__ and forward`methods.
"""

#####standard pytorch import
import torch
import torch.nn as nn
import torch.nn.functional as F  

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        ### here you have to initialize all parameters and submodules
        ### nn.Parameter is special type of tensor for storage of parameters
        ### there is many avaliable submodules - e.g. nn.Linear, nn.Conv2d, nn.ReLU.....

    def forward(self, x):
        ### here you have to implement all calculations of network
        return x

"""## Standard training procedure with optimizer

Implement network with module and apply it:
"""

import numpy as np
import matplotlib.pyplot as plt
from IPython.core.debugger import set_trace

import torch
import torch.nn as nn
import torch.nn.functional as F  
import torch.optim as optim


rng = np.random.RandomState(1)
X = 5 * rng.rand(150)
Y = np.sin(X).ravel()
Y[::10] += 3 * (0.5 - rng.rand(15))
Y=Y+0.1*np.random.randn(150)